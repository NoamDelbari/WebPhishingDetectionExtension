{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQel5-aEHStm"
      },
      "outputs": [],
      "source": [
        "import json, joblib, time, lightgbm as lgb\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from onnxmltools import convert_lightgbm\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import onnx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL_FEATURES = [\n",
        "    \"length_url\",\"length_hostname\",\"ip\",\"nb_dots\",\"nb_hyphens\",\"nb_at\",\n",
        "    \"nb_qm\",\"nb_and\",\"nb_or\",\"nb_eq\",\"nb_underscore\",\"nb_tilde\",\"nb_percent\",\n",
        "    \"nb_slash\",\"nb_star\",\"nb_colon\",\"nb_comma\",\"nb_semicolumn\",\"nb_dollar\",\n",
        "    \"nb_space\",\"nb_www\",\"nb_com\",\"nb_dslash\",\"http_in_path\",\"https_token\",\n",
        "    \"ratio_digits_url\",\"ratio_digits_host\",\"punycode\",\"port\",\"tld_in_path\",\n",
        "    \"tld_in_subdomain\",\"abnormal_subdomain\",\"nb_subdomains\",\"prefix_suffix\",\n",
        "    \"random_domain\",\"shortening_service\",\"path_extension\",\"length_words_raw\",\n",
        "    \"char_repeat\",\"shortest_words_raw\",\"shortest_word_host\",\n",
        "    \"shortest_word_path\",\"longest_words_raw\",\"longest_word_host\",\n",
        "    \"longest_word_path\",\"avg_words_raw\",\"avg_word_host\",\"avg_word_path\",\n",
        "    \"phish_hints\",\"domain_in_brand\",\"brand_in_subdomain\",\"brand_in_path\",\n",
        "    \"suspecious_tld\",\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zgl9f_jhHYMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL_FEATURES = [c.strip() for c in URL_FEATURES]        # tidy\n",
        "\n",
        "URL_COLS     = slice(0, 53)   # after IGNORE filtering\n",
        "CONTENT_COLS = slice(56, 80)               # after drop(IGNORE)\n"
      ],
      "metadata": {
        "id": "doGDiMqsHfmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResTiny(nn.Module):\n",
        "    def __init__(self, d_in=24):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_in, 96)\n",
        "        self.fc2 = nn.Linear(96, 48)\n",
        "        self.res = nn.Linear(d_in, 48, bias=False)   # match dims for add\n",
        "        self.fc3 = nn.Linear(48, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.out = nn.Linear(12, 1)\n",
        "        self.act, self.drop = nn.ReLU(), nn.Dropout(0.30)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.drop(self.act(self.fc1(x)))\n",
        "        h = self.drop(self.act(self.fc2(h) + self.res(x)))  # skip-add\n",
        "        h = self.drop(self.act(self.fc3(h)))\n",
        "        h = self.act(self.fc4(h))\n",
        "        return torch.sigmoid(self.out(h))\n",
        "\n"
      ],
      "metadata": {
        "id": "ZiT-9uZWHh8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def focal_loss(p, y, α=0.25, γ=2.0, eps=1e-6):\n",
        "    p = torch.clamp(p, eps, 1.0-eps)\n",
        "    ce = -(y*torch.log(p) + (1-y)*torch.log(1-p))\n",
        "    pt = torch.where(y==1, p, 1-p)\n",
        "    return (α * (1-pt)**γ * ce).mean()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNzbrbbfHlcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_stats(loader, model, lossf, dev):\n",
        "    model.eval()\n",
        "    losses, correct, total = [], 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(dev), yb.to(dev)\n",
        "            preds = model(xb)\n",
        "            losses.append(lossf(preds, yb).item())\n",
        "            correct += ((preds >= 0.5) == (yb >= 0.5)).sum().item()\n",
        "            total   += yb.size(0)\n",
        "    return np.mean(losses), correct / total\n",
        "\n"
      ],
      "metadata": {
        "id": "MGqkDCK4HnQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one(\n",
        "        name,                 # \"url_model\" | \"content_model\"\n",
        "        X, y, outdir,         # data slice + labels\n",
        "        *,                    # force keyword args after here\n",
        "        build_fn,             # callable(d_in) -> nn.Module  (required)\n",
        "        loss_type=\"bce\",      # \"bce\" | \"focal\"\n",
        "        pos_w=1.0,            # pos-class weight for BCE\n",
        "        patience=8,           # early-stop patience\n",
        "        batch=256, epochs=60  # training schedule\n",
        "    ):\n",
        "\n",
        "    scaler = StandardScaler();  X = scaler.fit_transform(X)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(\n",
        "        X, y, test_size=.20, stratify=y, random_state=42)\n",
        "\n",
        "    # ------------- bring these two lines back -----------------\n",
        "    tr = DataLoader(\n",
        "        TensorDataset(torch.tensor(Xtr, dtype=torch.float32),\n",
        "                      torch.tensor(ytr, dtype=torch.float32).unsqueeze(1)),\n",
        "        batch_size=256, shuffle=True)\n",
        "\n",
        "    va = DataLoader(\n",
        "        TensorDataset(torch.tensor(Xva, dtype=torch.float32),\n",
        "                      torch.tensor(yva, dtype=torch.float32).unsqueeze(1)),\n",
        "        batch_size=256)\n",
        "    # ----------------------------------------------------------\n",
        "\n",
        "    # pick architecture\n",
        "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if build_fn is None:\n",
        "        model = WebPhishCNN(X.shape[1])              # default\n",
        "    else:\n",
        "        model = build_fn(X.shape[1])\n",
        "    opt   = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "    pos_weight = torch.tensor([pos_w], device=dev)\n",
        "    if loss_type == \"focal\":\n",
        "        def lossf(pred, tgt): return focal_loss(pred, tgt, α=0.25, γ=2.0)\n",
        "    else:                                    # standard BCE\n",
        "        if pos_w != 1.0:\n",
        "            pos_weight = torch.tensor([pos_w], device=dev)\n",
        "            lossf = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        else:\n",
        "            lossf = nn.BCELoss()\n",
        "\n",
        "    # … rest of the loop unchanged …\n",
        "\n",
        "    best_loss, best_acc, wait = 9e9, 0.0, patience\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        for xb, yb in tr:\n",
        "            xb, yb = xb.to(dev), yb.to(dev)\n",
        "            opt.zero_grad()\n",
        "            loss = lossf(model(xb), yb)\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "        v_loss, v_acc = epoch_stats(va, model, lossf, dev)\n",
        "        print(f\"{name:14} ep{ep:02d}  loss={v_loss:.4f}  acc={v_acc*100:5.2f}%\")\n",
        "\n",
        "        if v_loss < best_loss:\n",
        "            best_loss, best_acc, wait = v_loss, v_acc, patience\n",
        "            torch.save(model.state_dict(), outdir / f\"{name}.pt\")\n",
        "        else:\n",
        "            wait -= 1\n",
        "            if wait == 0:\n",
        "                break\n",
        "\n",
        "    print(f\"✔ {name} best loss={best_loss:.4f} acc={best_acc*100:5.2f}%\\n\")\n",
        "\n",
        "    # save scaler\n",
        "    (outdir/f\"scaler_{name}.json\").write_text(\n",
        "        json.dumps({\"mean\": scaler.mean_.tolist(),\n",
        "                    \"std\":  scaler.scale_.tolist()}))\n",
        "\n",
        "    # export ONNX\n",
        "    dummy = torch.randn(1, X.shape[1])\n",
        "    torch.onnx.export(model.cpu(), dummy, outdir/f\"{name}.onnx\",\n",
        "                      input_names=[\"x\"], output_names=[\"y\"],\n",
        "                      dynamic_axes={\"x\": {0: \"b\"}, \"y\": {0: \"b\"}},\n",
        "                      opset_version=12)\n",
        "\n"
      ],
      "metadata": {
        "id": "GxaZaR8lHonz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    out = Path(\"outdir\"); out.mkdir(exist_ok=True)\n",
        "    df = pd.read_csv(\"csv_path\")\n",
        "\n",
        "    raw = df.copy()\n",
        "    y   = (raw[\"status\"] == \"phishing\").astype(np.float32).values\n",
        "    raw_dropped = raw.drop(columns=IGNORE)\n",
        "    missing = [c for c in URL_FEATURES if c not in raw_dropped.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"CSV is missing URL feature(s): {raw_dropped}\")\n",
        "    X   = raw_dropped.values.astype(np.float32)\n",
        "    cols_after_drop = raw_dropped.columns\n",
        "\n",
        "    train_one(\n",
        "        \"url_model\", X[:, URL_COLS], y, out,\n",
        "        build_fn=lambda d: ResTiny(d),\n",
        "        loss_type=\"focal\", pos_w=1.0, patience=5)\n",
        "\n",
        "\n",
        "    X   = df.values.astype(np.float32)[:, CONTENT_COLS]\n",
        "    y   = (pd.read_csv(\"csv_path\")[\"status\"]==\"phishing\").astype(np.int32).values\n",
        "\n",
        "    # scale numeric cols (tree depth is small, scaling helps a bit)\n",
        "    scaler = StandardScaler().fit(X)\n",
        "    X = scaler.transform(X)\n",
        "\n",
        "    Xtr, Xva, ytr, yva = train_test_split(\n",
        "        X, y, test_size=.20, stratify=y, random_state=42)\n",
        "\n",
        "    lgbm = lgb.LGBMClassifier(\n",
        "            n_estimators=400, learning_rate=0.05,\n",
        "            num_leaves=64, subsample=0.8, colsample_bytree=0.8,\n",
        "            max_depth=-1, objective=\"binary\", n_jobs=-1)\n",
        "\n",
        "    lgbm.fit(Xtr, ytr, eval_set=[(Xva,yva)], eval_metric=\"auc\")\n",
        "\n",
        "    print(\"val-accuracy:\",\n",
        "          (lgbm.predict(Xva) == yva).mean().round(4))\n",
        "\n",
        "    # Save model + scaler\n",
        "    joblib.dump(lgbm,  out/\"content_lgbm.pkl\")\n",
        "    json.dump({\"mean\": scaler.mean_.tolist(),\n",
        "              \"std\":  scaler.scale_.tolist()},\n",
        "              open(out/\"scaler_content_lgbm.json\",\"w\"))\n",
        "\n",
        "    initial = [(\"x\", FloatTensorType([None, X.shape[1]]))]\n",
        "    onx = convert_lightgbm(\n",
        "            lgbm,                    # fitted model\n",
        "            initial_types=initial,\n",
        "            zipmap=False,            # disable ZipMap so ORT gets a tensor\n",
        "            target_opset=13)\n",
        "    open(f\"{out}/content_lgbm.onnx\",\"wb\").write(onx.SerializeToString())\n",
        "\n",
        "    Path(f\"{out}/content_lgbm.onnx\").write_bytes(onx.SerializeToString())\n",
        "    print(\"✔  ZipMap removed:\",\n",
        "          all(n.op_type != \"ZipMap\" for n in onx.graph.node))\n",
        "\n",
        "    m = onnx.load(f\"{out}/content_lgbm.onnx\")\n",
        "    assert \"ZipMap\" not in [n.op_type for n in m.graph.node]\n",
        "    print(\"✔ tensor output, ready for onnxruntime-web\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lbV8s-8yHrJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "jpsbCE95Hu33"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}